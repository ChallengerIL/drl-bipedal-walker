import os
import numpy as np
import tensorflow as tf
import gymnasium as gym
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import optuna

tf.keras.utils.disable_interactive_logging()


class SumTree:

    def __init__(self, max_size):
        self.max_size = max_size
        self.tree_size = 2 * max_size - 1
        self.tree = np.zeros(self.tree_size)
        self.data = np.zeros(max_size, dtype=object)
        self.data_pointer = 0
        self.is_filled = False

    def add(self, priority, data):
        tree_index = self.data_pointer + self.max_size - 1
        self.data[self.data_pointer] = data
        self.update(tree_index, priority)

        self.data_pointer = (self.data_pointer + 1) % self.max_size
        if self.data_pointer == 0:
            self.is_filled = True

    def update(self, tree_index, priority):
        change = priority - self.tree[tree_index]
        self.tree[tree_index] = priority

        while tree_index != 0:
            tree_index = (tree_index - 1) // 2
            self.tree[tree_index] += change

    def get_total_priority(self):
        return self.tree[0]

    def sample(self, value):
        parent_index = 0

        while True:
            left_child_index = 2 * parent_index + 1
            right_child_index = left_child_index + 1

            if left_child_index >= self.tree_size:
                tree_index = parent_index
                break

            if value <= self.tree[left_child_index]:
                parent_index = left_child_index
            else:
                value -= self.tree[left_child_index]
                parent_index = right_child_index

        data_index = tree_index - self.max_size + 1
        return (tree_index, self.tree[tree_index], self.data[data_index])


class PrioritizedReplayBuffer:

    def __init__(self, max_size=200000, alpha=0.6, beta=0.4, epsilon=1e-6):
        self.sum_tree = SumTree(max_size)
        self.max_size = max_size
        self.alpha = alpha
        self.beta = beta
        self.epsilon = epsilon

    def add(self, transition, priority=None):
        if priority is None:
            priority = self.sum_tree.get_total_priority() + self.epsilon

        priority = (priority + self.epsilon) ** self.alpha
        self.sum_tree.add(priority, transition)

    def sample(self, batch_size):
        total_priority = self.sum_tree.get_total_priority()
        batch_transitions = []
        batch_indices = []
        batch_weights = []

        segment = total_priority / batch_size

        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)

            value = np.random.uniform(a, b)
            tree_index, priority, transition = self.sum_tree.sample(value)

            prob = priority / total_priority
            weight = (prob * len(self.sum_tree.data)) ** -self.beta

            batch_transitions.append(transition)
            batch_indices.append(tree_index)
            batch_weights.append(weight)

        batch_weights = np.array(batch_weights) / max(batch_weights)

        return batch_transitions, batch_indices, batch_weights

    def update_priorities(self, indices, new_priorities):
        for index, priority in zip(indices, new_priorities):
            priority = (priority + self.epsilon) ** self.alpha
            self.sum_tree.update(index, priority)

    def __len__(self):
        return len(self.sum_tree.data)


class IntelligentArchitectureSearch:

    def __init__(self, state_dim, action_dim,
                 min_layers=2, max_layers=5,
                 min_nodes=64, max_nodes=512):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.min_layers = min_layers
        self.max_layers = max_layers
        self.min_nodes = min_nodes
        self.max_nodes = max_nodes

    def optimize_architecture(self, env, n_trials=50):

        def objective(trial):
            actor_num_layers = trial.suggest_int('actor_num_layers', self.min_layers, self.max_layers)
            critic_num_layers = trial.suggest_int('critic_num_layers', self.min_layers, self.max_layers)

            actor_layers_config = [
                trial.suggest_int(f'actor_layer_{i}_nodes',
                                  self.min_nodes,
                                  self.max_nodes,
                                  log=True)
                for i in range(actor_num_layers)
            ]

            critic_layers_config = [
                trial.suggest_int(f'critic_layer_{i}_nodes',
                                  self.min_nodes,
                                  self.max_nodes,
                                  log=True)
                for i in range(critic_num_layers)
            ]

            exploration_strategies = ['adaptive', 'probabilistic', 'decaying']
            strategy = trial.suggest_categorical('exploration_strategy', exploration_strategies)
            learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
            gamma = trial.suggest_float('gamma', 0.90, 0.9999)

            agent = AdaptivePolicyNetwork(
                self.state_dim,
                self.action_dim,
                learning_rate=learning_rate,
                gamma=gamma,
                exploration_strategy=strategy,
                actor_layers_config=actor_layers_config,
                critic_layers_config=critic_layers_config
            )

            total_rewards = self._evaluate_agent(agent, env)
            return np.mean(total_rewards)

        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)

        best_params = study.best_params
        return {
            'actor_config': [
                best_params[f'actor_layer_{i}_nodes']
                for i in range(best_params['actor_num_layers'])
            ],
            'critic_config': [
                best_params[f'critic_layer_{i}_nodes']
                for i in range(best_params['critic_num_layers'])
            ],
            'exploration_strategy': best_params['exploration_strategy'],
            'learning_rate': best_params['learning_rate'],
            'gamma': best_params['gamma'],
            'best_reward': study.best_value
        }

    def _evaluate_agent(self, agent, env, num_episodes=30):
        rewards = []

        for _ in range(num_episodes):
            state, _ = env.reset()
            total_reward = 0
            done = False
            steps = 0

            while not done and steps < 2000:
                action = agent.select_action(state, training=True)
                next_state, reward, terminated, truncated, _ = env.step(action)

                done = terminated or truncated
                agent.train(state, action, reward, next_state, done)

                state = next_state
                total_reward += reward
                steps += 1

            rewards.append(total_reward)

        return rewards


class Normalizer:

    def __init__(self, decay_rate=0.99):
        self.mean = 0
        self.var = 1
        self.count = 0
        self.decay_rate = decay_rate

    def normalize(self, reward):
        self.count += 1
        delta = reward - self.mean
        self.mean += delta / self.count
        self.var = self.decay_rate * self.var + (1 - self.decay_rate) * (delta ** 2)
        return (reward - self.mean) / (np.sqrt(self.var) + 1e-8)


class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

    def __init__(self,
                 initial_learning_rate=1e-4,
                 decay_steps=10000,
                 decay_rate=0.9,
                 warmup_steps=1000):
        super().__init__()
        self.initial_learning_rate = initial_learning_rate
        self.decay_steps = decay_steps
        self.decay_rate = decay_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        warmup_lr = (self.initial_learning_rate * tf.cast(step, tf.float32) /
                     tf.cast(self.warmup_steps, tf.float32))

        decay_lr = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=self.initial_learning_rate,
            decay_steps=self.decay_steps,
            decay_rate=self.decay_rate
        )(step - self.warmup_steps)

        return tf.cond(
            step < self.warmup_steps,
            lambda: warmup_lr,
            lambda: tf.maximum(decay_lr, 1e-5)
        )

    def get_config(self):
        return {
            "initial_learning_rate": self.initial_learning_rate,
            "decay_steps": self.decay_steps,
            "decay_rate": self.decay_rate,
            "warmup_steps": self.warmup_steps
        }


def create_learning_rate_schedule(
        initial_lr=1e-4,
        decay_steps=10000,
        decay_rate=0.9,
        warmup_steps=1000
):
    return CustomLearningRateSchedule(
        initial_learning_rate=initial_lr,
        decay_steps=decay_steps,
        decay_rate=decay_rate,
        warmup_steps=warmup_steps
    )


class AdaptivePolicyNetwork:

    def __init__(self, state_dim, action_dim,
                 learning_rate=1e-4,
                 tau=0.005,
                 gamma=0.99,
                 exploration_strategy='enhanced',
                 batch_size=128,
                 buffer_size=200000,
                 actor_layers_config=None,
                 critic_layers_config=None,
                 lambda_=0.95):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.exploration_strategy = exploration_strategy
        self.lambda_ = lambda_

        self.actor_layers_config = actor_layers_config
        self.critic_layers_config = critic_layers_config

        lr_schedule = create_learning_rate_schedule(
            initial_lr=learning_rate,
            decay_steps=10000,
            decay_rate=0.9,
            warmup_steps=1000
        )

        self.actor = self._build_actor_network()
        self.actor_optimizer = keras.optimizers.Adam(
            learning_rate=lr_schedule,
            clipnorm=1.0
        )

        self.critic = self._build_critic_network()
        self.critic_optimizer = keras.optimizers.Adam(
            learning_rate=lr_schedule,
            clipnorm=1.0
        )

        self.actor_target = keras.models.clone_model(self.actor)
        self.critic_target = keras.models.clone_model(self.critic)
        self.actor_target.set_weights(self.actor.get_weights())
        self.critic_target.set_weights(self.critic.get_weights())

        self.exploration_strategy = exploration_strategy

        self.tau = tau
        self.gamma = gamma
        self.noise_decay = 0.995
        self.noise_min = 0.1
        self.current_noise = 1.0

        self.buffer_size = buffer_size
        self.buffer = PrioritizedReplayBuffer(
            max_size=self.buffer_size,
            alpha=0.6,
            beta=0.4,
            epsilon=1e-6
        )
        self.batch_size = batch_size
        self.beta = 0.4
        self.reward_decay = 0.99

        self.normalizer = Normalizer()

        self.training_steps = 0
        self.performance_threshold = 150
        self.exploration_rate = 1.0
        self.learning_rate_schedule = create_learning_rate_schedule(...)

    def _build_actor_network(self):
        inputs = layers.Input(shape=(self.state_dim,))
        x = inputs

        for num_nodes in self.actor_layers_config:
            x = layers.Dense(num_nodes, activation='relu', kernel_initializer='he_normal')(x)
            x = layers.LayerNormalization()(x)

        outputs = layers.Dense(self.action_dim, activation='tanh')(x)
        return keras.Model(inputs=inputs, outputs=outputs)

    def _build_critic_network(self):
        state_input = layers.Input(shape=(self.state_dim,))
        action_input = layers.Input(shape=(self.action_dim,))

        x = layers.Concatenate()([state_input, action_input])

        for num_nodes in self.critic_layers_config:
            x = layers.Dense(num_nodes, activation='relu', kernel_initializer='he_normal')(x)
            x = layers.LayerNormalization()(x)

        outputs = layers.Dense(1)(x)

        return keras.Model(inputs=[state_input, action_input], outputs=outputs)

    def select_action(self, state, training=True):
        state = np.reshape(state, [1, self.state_dim])
        action = self.actor.predict(state)[0]

        if training:
            action = self._advanced_exploration(action)

        return np.clip(action, -1, 1)

    def _advanced_exploration(self, action):
        if np.random.random() < 0.2:
            noise_type = np.random.choice(['gaussian', 'cauchy', 'uniform'])

            if noise_type == 'gaussian':
                action += np.random.normal(0, 0.2, action.shape)
            elif noise_type == 'cauchy':
                action += np.random.standard_cauchy(action.shape) * 0.1
            else:
                action += np.random.uniform(-0.3, 0.3, action.shape)
        return action

    def adjust_noise_based_on_performance(self, recent_rewards):
        avg_reward = np.mean(recent_rewards)

        if avg_reward > 150:
            self.current_noise = max(self.noise_min, self.current_noise * 0.99)
        elif avg_reward < 50:
            self.current_noise = min(1.0, self.current_noise * 1.05)

    def train(self, state, action, reward, next_state, done, recent_rewards=None):
        normalized_reward = self.normalizer.normalize(reward)
        transition = (state, action, normalized_reward, next_state, done)

        initial_priority = 1.0
        self.buffer.add(transition, initial_priority)

        if len(self.buffer) < self.batch_size:
            return

        batch, indices, weights = self.buffer.sample(self.batch_size)
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*batch)

        batch_states = tf.convert_to_tensor(np.array(batch_states), dtype=tf.float32)
        batch_actions = tf.convert_to_tensor(np.array(batch_actions), dtype=tf.float32)
        batch_rewards = tf.convert_to_tensor(np.array(batch_rewards), dtype=tf.float32)
        batch_next_states = tf.convert_to_tensor(np.array(batch_next_states), dtype=tf.float32)
        batch_dones = tf.convert_to_tensor(np.array(batch_dones), dtype=tf.float32)
        weights = tf.convert_to_tensor(weights, dtype=tf.float32)

        with tf.GradientTape() as tape:
            target_actions = self.actor_target(batch_next_states)
            target_critic_values = tf.squeeze(self.critic_target([batch_next_states, target_actions]), axis=-1)

            critic_values = tf.squeeze(self.critic([batch_states, batch_actions]), axis=-1)
            td_residual = batch_rewards + self.gamma * (1 - batch_dones) * target_critic_values - critic_values

            critic_loss = tf.reduce_mean(weights * tf.square(td_residual))

            new_priorities = np.abs(td_residual.numpy()) + 1e-6
            self.buffer.update_priorities(indices, new_priorities)

            advantages = self._compute_gae(td_residual, batch_next_states, batch_dones)

        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

        with tf.GradientTape() as tape:
            actions_pred = self.actor(batch_states)
            critic_values = self.critic([batch_states, actions_pred])

            entropy = -tf.reduce_mean(actions_pred * tf.math.log(actions_pred + 1e-6))
            adaptive_entropy_coef = tf.maximum(0.01, 1.0 / (1.0 + self.training_steps * 0.001))

            actor_loss = -tf.reduce_mean(critic_values * advantages) + adaptive_entropy_coef * entropy

        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))

        self._soft_update(self.actor, self.actor_target)
        self._soft_update(self.critic, self.critic_target)

        if recent_rewards is not None:
            self.adjust_noise_based_on_performance(recent_rewards)

    def _compute_gae(self, td_residuals, next_states, done_flags):
        advantages = np.zeros_like(td_residuals)
        last_advantage = 0

        for t in reversed(range(len(td_residuals))):
            if done_flags[t]:
                last_advantage = 0

            last_advantage = td_residuals[t] + self.gamma * self.lambda_ * last_advantage
            advantages[t] = last_advantage

        return advantages

    def _soft_update(self, model, target_model):
        model_weights = model.get_weights()
        target_model_weights = target_model.get_weights()

        for i in range(len(model_weights)):
            target_model_weights[i] = (
                    self.tau * model_weights[i] + (1 - self.tau) * target_model_weights[i]
            )

        target_model.set_weights(target_model_weights)

    def adjust_hyperparameters(self, recent_rewards):
        self.training_steps += 1
        mean_reward = np.mean(recent_rewards)

        if mean_reward > self.performance_threshold:
            self.exploration_rate = max(0.1, self.exploration_rate * 0.99)
            self.learning_rate_schedule.initial_learning_rate *= 1.01
        elif mean_reward < self.performance_threshold * 0.5:
            self.exploration_rate = min(1.0, self.exploration_rate * 1.1)
            self.learning_rate_schedule.initial_learning_rate *= 0.99


def train_bipedal_walker(env, agent, num_episodes=3000):
    reward_history = []
    noise_history = []
    episode_length_history = []
    best_mean_reward = float('-inf')

    for episode in range(num_episodes):
        state, _ = env.reset()
        total_reward = 0
        done = False
        steps = 0

        while not done:
            action = agent.select_action(state, training=True)
            next_state, reward, terminated, truncated, _ = env.step(action)

            done = terminated or truncated
            agent.train(state, action, reward, next_state, done,
                        recent_rewards=reward_history[-100:] if len(reward_history) >= 100 else None)

            state = next_state
            total_reward += reward
            steps += 1

            if steps > 2000:
                break

        reward_history.append(total_reward)
        noise_history.append(agent.current_noise)
        episode_length_history.append(steps)

        # if (episode + 1) % 10 == 0:
        mean_reward = np.mean(reward_history[-100:])
        print(
            f"Episode {episode + 1}: Total Reward = {total_reward:.2f}, "
            f"Mean Reward (100 episodes) = {mean_reward:.2f}, "
            f"Steps = {steps}, Noise = {agent.current_noise:.4f}"
        )

        if len(reward_history) >= 100:
            current_mean_reward = np.mean(reward_history[-100:])
            agent.adjust_hyperparameters(current_mean_reward)

            if current_mean_reward > best_mean_reward:
                best_mean_reward = current_mean_reward
                agent.actor.save(f'results/best_actor.keras')
                agent.critic.save(f'results/best_critic.keras')

        if (episode + 1) % 200 == 0:
            agent.actor.save(f'results/actor_ep{episode + 1}.keras')
            agent.critic.save(f'results/critic_ep{episode + 1}.keras')

    return {
        'reward_history': reward_history,
        'noise_history': noise_history,
        'episode_length_history': episode_length_history
    }


def run_bipedal_walker_experiment():
    os.makedirs('results', exist_ok=True)
    env = gym.make('BipedalWalker-v3')

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    architecture_optimizer = IntelligentArchitectureSearch(state_dim, action_dim)
    best_config = architecture_optimizer.optimize_architecture(env, n_trials=30)

    print("Best Architecture Found:")
    print("Actor Config:", best_config['actor_config'])
    print("Critic Config:", best_config['critic_config'])
    print("Exploration Strategy:", best_config['exploration_strategy'])
    print("Learning Rate:", best_config['learning_rate'])
    print("Gamma:", best_config['gamma'])

    agent = AdaptivePolicyNetwork(
        state_dim,
        action_dim,
        learning_rate=best_config['learning_rate'],
        gamma=best_config['gamma'],
        exploration_strategy=best_config['exploration_strategy'],
        actor_layers_config=best_config['actor_config'],
        critic_layers_config=best_config['critic_config']
    )

    results = train_bipedal_walker(env, agent)

    plt.figure(figsize=(15, 10))
    plt.subplot(2, 2, 1)
    plt.plot(results['reward_history'])
    plt.title('Reward Progress During Training')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')

    plt.subplot(2, 2, 2)
    plt.plot(results['noise_history'])
    plt.title('Exploration Noise Decay')
    plt.xlabel('Episode')
    plt.ylabel('Noise Level')

    plt.subplot(2, 2, 3)
    plt.plot(results['episode_length_history'])
    plt.title('Episode Length')
    plt.xlabel('Episode')
    plt.ylabel('Steps')

    plt.subplot(2, 2, 4)
    plt.plot(np.convolve(results['reward_history'], np.ones(50) / 50, mode='valid'))
    plt.title('Moving Average of Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Moving Average Reward')

    plt.tight_layout()
    plt.savefig('results/training_progress.png')

    test_episodes = 20
    test_rewards = []
    test_lengths = []

    for _ in range(test_episodes):
        state, _ = env.reset()
        total_test_reward = 0
        episode_steps = 0
        done = False

        while not done:
            action = agent.select_action(state, training=False)
            state, reward, terminated, truncated, _ = env.step(action)
            total_test_reward += reward
            episode_steps += 1
            done = terminated or truncated

        test_rewards.append(total_test_reward)
        test_lengths.append(episode_steps)

    print("\nTest Results:")
    print(f"Average Test Reward: {np.mean(test_rewards):.2f}")
    print(f"Test Reward Std Dev: {np.std(test_rewards):.2f}")
    print(f"Average Episode Length: {np.mean(test_lengths):.2f}")
    print(f"Episode Length Std Dev: {np.std(test_lengths):.2f}")

    env.close()

    return results


if __name__ == '__main__':
    run_bipedal_walker_experiment()
